{
  "categories": [
    { "id": "architecture", "label": "Architecture", "color": "#6366f1" },
    { "id": "inference", "label": "Inference", "color": "#8b5cf6" },
    { "id": "memory", "label": "Memory", "color": "#ec4899" },
    { "id": "training", "label": "Training", "color": "#10b981" },
    { "id": "parallelism", "label": "Parallelism", "color": "#f59e0b" },
    { "id": "observability", "label": "Observability", "color": "#06b6d4" },
    { "id": "retrieval", "label": "Retrieval/RAG", "color": "#14b8a6" }
  ],
  "nodes": [
    {
      "id": "multi_head_attention",
      "name": "Multi-Head Attention",
      "category": "architecture",
      "description": "Parallel attention computations across different representation subspaces. Projects Q, K, V into h heads.",
      "technical": "Each head projects Q, K, V independently and computes scaled dot-product attention. Original paper uses 8 heads stacked 6 times for depth.",
      "magnitudes": "Typically h=8 heads, O(n² * d) complexity per head",
      "tradeoffs": "More heads → richer representations but higher memory/compute cost",
      "references": ["Vaswani et al. 2017", "https://www.datacamp.com/tutorial/how-transformers-work"],
      "sourceLines": [3]
    },
    {
      "id": "scaled_dot_product",
      "name": "Scaled Dot-Product Attention",
      "category": "architecture",
      "description": "Core attention mechanism: QK^T scaled by sqrt(d_k) to normalize variance, then softmax.",
      "technical": "Dividing by sqrt(d_k) keeps variance ~1, preventing softmax saturation. Variance of sum of d_k products is proportional to d_k.",
      "magnitudes": "sqrt(d_k) scaling factor, O(n²) for sequence length n",
      "tradeoffs": "Without scaling, softmax becomes too peaked (Dirac-style), reducing gradient flow",
      "sourceLines": [5, 6, 7]
    },
    {
      "id": "layer_norm",
      "name": "Layer Normalization",
      "category": "architecture",
      "description": "Normalizes each vector to mean=0, variance=1 per sample, not across batch like BatchNorm.",
      "technical": "nn.LayerNorm(d_model) normalizes each d_model dimension individually. Better for variable-length sequences than BatchNorm.",
      "magnitudes": "Applied per token vector independently",
      "tradeoffs": "Stabilizes training (less gradient explosion), faster convergence. Pre-LayerNorm gives identity gradient in residual connections.",
      "references": ["Ba et al. 2016"],
      "sourceLines": [10, 11, 12, 13, 14, 15, 16, 17, 18]
    },
    {
      "id": "gelu",
      "name": "GELU Activation",
      "category": "architecture",
      "description": "Gaussian Error Linear Unit: x * Φ(x), smooth non-linearity used in transformers.",
      "technical": "Smoother than ReLU, incorporates probabilistic interpretation. GELU(x) = x * P(X ≤ x) where X ~ N(0,1).",
      "tradeoffs": "More expressive than ReLU but slightly more computation",
      "references": ["Hendrycks & Gimpel 2016"],
      "sourceLines": [164]
    },
    {
      "id": "residual_connections",
      "name": "Residual Connections",
      "category": "architecture",
      "description": "Skip connections that add input to output of sublayer, enabling gradient flow through deep networks.",
      "technical": "Pre-LayerNorm architecture gives identity gradient through residual path, critical for training stability.",
      "magnitudes": "Used in every transformer block (2x per block: attention + FFN)",
      "tradeoffs": "Enables very deep networks but increases memory (must store activations)",
      "sourceLines": [18]
    },
    {
      "id": "kv_cache",
      "name": "KV Caching",
      "category": "inference",
      "description": "Stores previously computed Key and Value matrices during autoregressive generation to avoid recomputation.",
      "technical": "In causal inference, K and V from previous tokens are reused when computing attention for new query. Only compute new K, V for current token.",
      "magnitudes": "Memory: O(n * d * 2 * num_layers) for sequence length n. Speeds up inference ~2-3x.",
      "tradeoffs": "Massive latency reduction but increases memory requirements linearly with sequence length",
      "references": ["https://huggingface.co/blog/not-lain/kv-caching", "https://medium.com/@joaolages/kv-caching-explained-276520203249"],
      "sourceLines": [51, 52, 53, 54]
    },
    {
      "id": "flash_attention",
      "name": "Flash Attention",
      "category": "inference",
      "description": "Memory-efficient attention that fuses operations and minimizes HBM reads/writes.",
      "technical": "Loads Q, K, V once, fuses attention operations (QK^T, softmax, multiply by V), writes back. Reduces memory bandwidth bottleneck.",
      "magnitudes": "~2-4x faster than standard attention, enables 2-4x longer sequences",
      "tradeoffs": "Hardware-specific optimizations (requires CUDA), more complex implementation",
      "references": ["Dao et al. 2022", "https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention"],
      "sourceLines": [97, 98, 99]
    },
    {
      "id": "sparse_attention",
      "name": "Sparse Attention",
      "category": "inference",
      "description": "Restricts attention to important token pairs instead of all-to-all, reducing O(n²) complexity.",
      "technical": "Variants: sliding window (local), global tokens (fixed), random attention. Question: do all tokens really need to attend to all others?",
      "magnitudes": "Can reduce complexity to O(n * sqrt(n)) or O(n * log(n)) depending on pattern",
      "tradeoffs": "Lower memory/compute but may miss long-range dependencies. Must carefully design sparsity pattern.",
      "references": ["BigBird", "https://huggingface.co/blog/big-bird"],
      "sourceLines": [97, 101, 102, 103, 104, 105, 106]
    },
    {
      "id": "sliding_window_attention",
      "name": "Sliding Window Attention",
      "category": "inference",
      "description": "Restricts attention to a local window (e.g., previous 512 tokens only).",
      "technical": "Each token attends only to k previous tokens. Linear memory O(n*k) instead of O(n²).",
      "magnitudes": "Typical window: 512-2048 tokens",
      "tradeoffs": "Loses true long-range dependencies but drastically reduces memory",
      "sourceLines": [65]
    },
    {
      "id": "context_window",
      "name": "Context Window",
      "category": "inference",
      "description": "Maximum sequence length the model can process. Limited by O(n²) attention memory.",
      "technical": "Not just about input size - QK^T attention matrix grows quadratically. Memory constraint, not just architecture.",
      "magnitudes": "GPT-3: 2048, GPT-4: 8k-32k, Claude: 100k+",
      "tradeoffs": "Larger context → better understanding but exponentially more memory/compute",
      "references": ["https://huggingface.co/blog/not-lain/kv-caching"],
      "sourceLines": [47, 48, 49, 57, 58, 60, 61, 62]
    },
    {
      "id": "batching_strategy",
      "name": "Batching Strategy",
      "category": "inference",
      "description": "Grouping multiple requests together for parallel processing on GPU.",
      "technical": "Dynamic batching: group requests within time window. Avoid padding large sequences (wastes memory).",
      "magnitudes": "Typical batch sizes: 8-64 for inference",
      "tradeoffs": "Small batch → lower latency, lower throughput. Large batch → higher throughput, higher latency.",
      "sourceLines": [187, 190, 191, 192, 193, 194]
    },
    {
      "id": "attention_complexity",
      "name": "Attention O(n²) Complexity",
      "category": "memory",
      "description": "Attention matrix QK^T has quadratic memory and compute requirements in sequence length.",
      "technical": "For sequence length n and dimension d: memory O(n²), compute O(n² * d). Fundamental bottleneck for long sequences.",
      "magnitudes": "For n=2048, d=768: ~3GB just for attention matrices",
      "tradeoffs": "All-to-all attention is powerful but doesn't scale to very long sequences",
      "sourceLines": [60, 61]
    },
    {
      "id": "oom",
      "name": "Out of Memory (OOM)",
      "category": "memory",
      "description": "Memory capacity failure - model/activations/KV cache exceed device memory.",
      "technical": "OOM is distinct from latency bottleneck. Can happen even with small sequences if model is huge. Common failure mode in training and long-context inference.",
      "magnitudes": "Typical GPU: 16-80GB. Large model + activations can easily exceed this.",
      "tradeoffs": "Must balance model size, batch size, sequence length, precision",
      "sourceLines": [110, 111, 112, 113, 114]
    },
    {
      "id": "activation_memory",
      "name": "Activation Memory",
      "category": "memory",
      "description": "Intermediate results from forward pass, must be kept for backpropagation.",
      "technical": "All layer outputs, attention scores, FFN intermediate states. Dominates memory in training. Can be reduced with gradient checkpointing.",
      "magnitudes": "Typically 10-20x model size during training",
      "tradeoffs": "Essential for backprop but massive memory cost. Gradient checkpointing trades compute for memory.",
      "sourceLines": [117]
    },
    {
      "id": "weight_memory",
      "name": "Weight Memory",
      "category": "memory",
      "description": "Memory for model parameters. Varies by precision (FP32, FP16, INT8).",
      "technical": "FP32: 4 bytes/param. FP16: 2 bytes/param. INT8: 1 byte/param. Inference weights ≠ training weights (training needs optimizer states).",
      "magnitudes": "7B model in FP32: 28GB. In FP16: 14GB. In INT8: 7GB.",
      "tradeoffs": "Lower precision → less memory but potential accuracy loss",
      "sourceLines": [71, 72, 73]
    },
    {
      "id": "gradient_memory",
      "name": "Gradient Memory",
      "category": "memory",
      "description": "Storage for gradients during backpropagation. Same size as model weights.",
      "technical": "Each parameter needs gradient storage. Plus optimizer states (momentum, variance for Adam) = 2x more memory.",
      "magnitudes": "Total training memory: weights + gradients + optimizer states + activations",
      "tradeoffs": "Can use gradient accumulation to reduce memory at cost of slower updates",
      "sourceLines": [74]
    },
    {
      "id": "mixed_precision",
      "name": "Mixed Precision Training",
      "category": "training",
      "description": "Use FP16 for compute, FP32 for weight updates. Reduces memory and speeds up training.",
      "technical": "Forward/backward in FP16. Master weights in FP32. Loss scaling prevents gradient underflow. Some layers (BatchNorm) stay FP32.",
      "magnitudes": "~2x memory reduction, 2-3x speedup on modern GPUs",
      "tradeoffs": "Must handle underflow with loss scaling. Small added complexity.",
      "references": ["Micikevicius et al. 2017"],
      "sourceLines": [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
    },
    {
      "id": "loss_scaling",
      "name": "Loss Scaling",
      "category": "training",
      "description": "Multiply loss by large constant to prevent gradient underflow in FP16.",
      "technical": "Scale loss by 1024 or similar. Compute gradients in FP16. Unscale before weight update. Preserves tiny gradients that would become 0 in FP16.",
      "magnitudes": "Typical scale factor: 128-1024",
      "tradeoffs": "Simple technique, essential for FP16 training. Must tune scale factor.",
      "sourceLines": [27, 28, 29, 30, 31]
    },
    {
      "id": "quantization",
      "name": "Quantization",
      "category": "training",
      "description": "Convert high-precision (FP32/FP16) to lower precision (INT8/INT4) to reduce memory and compute.",
      "technical": "PTQ (Post-Training Quantization): quantize after training. QAT (Quantization-Aware Training): simulate quantization during training for better accuracy.",
      "magnitudes": "INT8: 4x smaller than FP32. INT4: 8x smaller.",
      "tradeoffs": "PTQ is easy but may lose accuracy. QAT is expensive but maintains quality.",
      "references": ["Jacob et al. 2017"],
      "sourceLines": [43, 44, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]
    },
    {
      "id": "rlhf",
      "name": "RLHF (Reinforcement Learning from Human Feedback)",
      "category": "training",
      "description": "Train a reward model from human preferences, then optimize LLM policy using RL.",
      "technical": "1) Collect human rankings of outputs. 2) Train reward model to predict human preferences (Elo-style). 3) Use PPO/similar to optimize LLM. Works for subjective tasks (humor, helpfulness).",
      "magnitudes": "Typical: 1000 prompts, 5 rollouts each → 5000 samples for reward model",
      "tradeoffs": "Humans find ranking easier than generation. Enables optimization on unverifiable domains. But: expensive, risk of reward hacking.",
      "references": ["Christiano et al. 2017", "Ouyang et al. 2022 (InstructGPT)"],
      "sourceLines": [170, 171, 172, 173, 174, 175, 176, 177]
    },
    {
      "id": "post_training",
      "name": "Post-Training / Fine-Tuning",
      "category": "training",
      "description": "Train base model on conversational/instruction data to create a chatbot/assistant.",
      "technical": "Fine-tune pretrained model on dataset of instructions and responses. Includes supervised fine-tuning (SFT) and RLHF.",
      "magnitudes": "Typically 10k-100k instruction examples",
      "tradeoffs": "Relatively cheap vs pretraining. Aligns model to desired behavior.",
      "sourceLines": [166]
    },
    {
      "id": "tensor_parallelism",
      "name": "Tensor Parallelism (TP)",
      "category": "parallelism",
      "description": "Split individual layers (matrix multiplications) across GPUs. Each GPU computes a slice of weights.",
      "technical": "Each GPU stores fraction of parameters. Activations need all-reduce for synchronization. High inter-GPU communication per layer.",
      "magnitudes": "Typical: 2-8 GPUs per TP group",
      "tradeoffs": "Needed when single layer doesn't fit on one GPU. Adds communication latency but reduces per-GPU memory.",
      "sourceLines": [124, 125, 126, 127, 128, 129]
    },
    {
      "id": "pipeline_parallelism",
      "name": "Pipeline Parallelism (PP)",
      "category": "parallelism",
      "description": "Split model layers into sequential stages across GPUs. Mini-batches flow through pipeline.",
      "technical": "Each GPU holds subset of layers. Micro-batching reduces pipeline bubbles. Higher latency at pipeline start but good throughput.",
      "magnitudes": "Typical: 4-16 pipeline stages",
      "tradeoffs": "Enables extremely deep models. Pipeline bubbles waste compute but micro-batching helps.",
      "sourceLines": [131, 132, 133, 134, 135]
    },
    {
      "id": "data_parallelism",
      "name": "Data Parallelism (DP)",
      "category": "parallelism",
      "description": "Replicate full model on each GPU. Each processes different data. Average gradients across devices.",
      "technical": "Full model per GPU (unless using ZeRO sharding). Scales throughput linearly with batch size. Communication during gradient reduction.",
      "magnitudes": "Can scale to 100s-1000s of GPUs",
      "tradeoffs": "Simple, scales throughput well. But: full model per GPU (memory intensive). Often combined with TP/PP.",
      "sourceLines": [137, 138, 139, 140, 141]
    },
    {
      "id": "communication_overhead",
      "name": "Communication Overhead",
      "category": "parallelism",
      "description": "Time and bandwidth cost of moving tensors between devices instead of computing.",
      "technical": "Transfer gradients, activations, parameters between GPUs. Can dominate FLOPs at scale. Grows with model depth and parallelism degree.",
      "magnitudes": "Can be 30-50% of total time in large distributed training",
      "tradeoffs": "Fundamental bottleneck in scaling. Improved by better interconnects (NVLink, InfiniBand).",
      "sourceLines": [91, 92, 93, 94]
    },
    {
      "id": "flops",
      "name": "FLOPs",
      "category": "observability",
      "description": "Floating Point Operations - measure of computational work.",
      "technical": "FLOPs for transformer forward: ~6 * num_params * batch * seqlen. Backward: ~2x forward.",
      "magnitudes": "GPT-3 training: ~3.14e23 FLOPs",
      "tradeoffs": "FLOPs don't account for memory bandwidth, communication overhead. Can be misleading metric.",
      "sourceLines": [89]
    },
    {
      "id": "latency",
      "name": "Latency",
      "category": "observability",
      "description": "Response time for single request or token generation.",
      "technical": "Measured per request or per token. P99 latency: 99% of requests faster than this value. More important than mean for user experience.",
      "magnitudes": "Good token latency: 10-50ms. P99 should be <2x P50.",
      "tradeoffs": "Lower latency → better UX but may reduce throughput. Batching increases latency.",
      "sourceLines": [150, 179, 180, 181, 182, 183, 184, 185]
    },
    {
      "id": "throughput",
      "name": "Throughput",
      "category": "observability",
      "description": "Number of tokens processed per second across all requests.",
      "technical": "Tokens/sec or requests/sec. Higher throughput = better GPU utilization. Increased by batching.",
      "magnitudes": "Modern serving: 1000-10000 tokens/sec per GPU",
      "tradeoffs": "Large batches → high throughput but high latency. Fundamental tradeoff.",
      "sourceLines": [119, 120, 187]
    },
    {
      "id": "memorization",
      "name": "Memorization Detection",
      "category": "observability",
      "description": "Detecting when model outputs training data verbatim. Privacy and copyright concern.",
      "technical": "Exposure metric: fraction of n-grams from training corpus. Membership inference attacks. High-confidence extraction (temp=0, beam search).",
      "magnitudes": "Models can memorize 1-2% of training examples",
      "tradeoffs": "Larger models memorize more. Tradeoff between capability and privacy.",
      "references": ["Carlini et al. 2021"],
      "sourceLines": [196, 197, 198, 199, 200, 201]
    },
    {
      "id": "perplexity",
      "name": "Perplexity",
      "category": "observability",
      "description": "Exponentiated cross-entropy loss. Lower = better language model.",
      "technical": "PPL = exp(mean negative log-likelihood). Measures how 'surprised' model is by test data.",
      "magnitudes": "Good LM: PPL < 20. Poor: PPL > 100.",
      "tradeoffs": "Easy to compute but doesn't capture generation quality, safety, factuality.",
      "sourceLines": [168]
    },
    {
      "id": "bleu_rouge",
      "name": "BLEU / ROUGE",
      "category": "observability",
      "description": "N-gram overlap metrics for translation and summarization quality.",
      "technical": "BLEU: precision-focused (translation). ROUGE: recall-focused (summarization). Both use n-gram matching with reference text.",
      "magnitudes": "BLEU scores: 0-100, good translation >40. ROUGE: 0-1 scale.",
      "tradeoffs": "Easy to compute but poor correlation with human judgment for generation tasks. Better for MT than open-ended generation.",
      "sourceLines": [168]
    },
    {
      "id": "hallucination_detection",
      "name": "Hallucination Detection",
      "category": "observability",
      "description": "Identifying when model generates false or unsupported information.",
      "technical": "Semantic consistency checks (cross-compare with retrieval sources). Fact-checking models for entities/numbers. Confidence calibration.",
      "magnitudes": "State-of-art detectors: 70-85% accuracy",
      "tradeoffs": "Hard to automate reliably. Often needs human verification. Active research area.",
      "sourceLines": [153, 154, 155]
    },
    {
      "id": "toxicity_detection",
      "name": "Toxicity Detection",
      "category": "observability",
      "description": "Detecting offensive, biased, or harmful model outputs.",
      "technical": "Use classifiers (Perspective API, in-house models). Monitor distribution shifts. User feedback loops.",
      "magnitudes": "Perspective API: 0-1 toxicity score",
      "tradeoffs": "Classifiers can be biased themselves. Must balance safety and false positives.",
      "references": ["Perspective API"],
      "sourceLines": [156]
    },
    {
      "id": "numerical_instability",
      "name": "Numerical Instability",
      "category": "memory",
      "description": "Gradient saturation, NaN values, especially in FP16. Mitigated by layer norm, loss scaling.",
      "technical": "Multiple layers + FP16 can cause gradient underflow/overflow. LayerNorm stabilizes activations. Loss scaling preserves small gradients.",
      "magnitudes": "Without mitigation: NaN after 10-20 layers in FP16",
      "tradeoffs": "Must carefully engineer numerical stability for deep networks and mixed precision.",
      "sourceLines": [115]
    },
    {
      "id": "bm25",
      "name": "BM25 (Best Matching 25)",
      "category": "retrieval",
      "description": "Ranking algorithm used in search engines to determine document relevance to a query.",
      "technical": "Balances term frequency (TF) with inverse document frequency (IDF) and document length. Penalizes overly long documents and caps benefit of repeated terms. More nuanced than TF-IDF.",
      "magnitudes": "Used in Elasticsearch, Lucene, Solr. Standard for text search.",
      "tradeoffs": "Better than TF-IDF for varied document lengths, but still sparse (doesn't capture semantic meaning)",
      "references": ["Robertson & Zaragoza 2009"]
    },
    {
      "id": "tf_idf",
      "name": "TF-IDF",
      "category": "retrieval",
      "description": "Statistical measure evaluating word importance in a document relative to a corpus.",
      "technical": "Term Frequency (how often word appears) × Inverse Document Frequency (how rare the word is). Sparse representation: each document is a vector of word frequencies.",
      "magnitudes": "Classic IR method, still used for baseline comparisons",
      "tradeoffs": "Fast and interpretable but doesn't capture semantic similarity. 'king' and 'monarch' are unrelated.",
      "references": ["Salton & Buckley 1988"]
    },
    {
      "id": "dense_retrieval",
      "name": "Dense Retrieval",
      "category": "retrieval",
      "description": "Query and documents encoded as dense vectors (embeddings) for semantic similarity search.",
      "technical": "Use neural encoders (e.g., BERT, sentence transformers) to map text to continuous vector space. Similarity measured by cosine or dot product.",
      "magnitudes": "Vector dimensions: 384-1024 typical. Captures semantic meaning.",
      "tradeoffs": "Better semantic matching than sparse methods, but requires neural encoding (slower) and large vector indices",
      "references": ["Karpukhin et al. 2020 (DPR)"]
    },
    {
      "id": "sparse_retrieval",
      "name": "Sparse Retrieval",
      "category": "retrieval",
      "description": "Documents represented as high-dimensional sparse vectors based on exact term matching.",
      "technical": "TF-IDF, BM25 create vectors where most entries are 0 (only terms present are non-zero). Fast exact keyword matching.",
      "magnitudes": "Vocabulary size = vector dimension (10k-100k+)",
      "tradeoffs": "Very fast, good for exact matches, but misses semantic similarity. 'car' won't match 'automobile'.",
      "references": ["Classic IR methods"]
    },
    {
      "id": "hybrid_retrieval",
      "name": "Hybrid Retrieval",
      "category": "retrieval",
      "description": "Combines dense (semantic) and sparse (keyword) retrieval for best of both worlds.",
      "technical": "Run both BM25 and dense vector search, then merge results (e.g., reciprocal rank fusion). Balances exact matching with semantic understanding.",
      "magnitudes": "Typical: 70% weight on dense, 30% on sparse (tunable)",
      "tradeoffs": "Higher accuracy but ~2x compute cost (run both searches). Need to tune fusion weights.",
      "references": ["Hybrid search in Weaviate, Pinecone"]
    },
    {
      "id": "ann_search",
      "name": "Approximate Nearest Neighbors (ANN)",
      "category": "retrieval",
      "description": "Algorithms to quickly find nearest vectors without exhaustive search.",
      "technical": "Use indexing structures (HNSW graphs, IVF clusters, product quantization) to prune search space. Trade accuracy for speed.",
      "magnitudes": "FAISS can search billions of vectors in milliseconds. Typical recall@10: 95%+",
      "tradeoffs": "Approximate (not exact) but 100-1000x faster than brute force. Index build time can be long.",
      "references": ["Johnson et al. 2019 (FAISS)"]
    },
    {
      "id": "faiss",
      "name": "FAISS (Facebook AI Similarity Search)",
      "category": "retrieval",
      "description": "Highly optimized library for dense vector similarity search and clustering.",
      "technical": "Implements multiple ANN algorithms: flat (exact), IVF (clustering), HNSW (graph), PQ (quantization). GPU-accelerated.",
      "magnitudes": "Billion-scale vector search. Used at Meta, OpenAI, etc.",
      "tradeoffs": "Very fast and scalable but requires careful index tuning (many knobs to turn)",
      "references": ["https://github.com/facebookresearch/faiss", "Johnson et al. 2019"]
    },
    {
      "id": "hyde",
      "name": "HyDE (Hypothetical Document Embeddings)",
      "category": "retrieval",
      "description": "Generate hypothetical answer to query, then use its embedding for retrieval instead of query embedding.",
      "technical": "Query → LLM generates fake answer → Embed fake answer → Search for similar real docs. Bridges query-document gap.",
      "magnitudes": "Can improve retrieval recall by 10-20% on certain datasets",
      "tradeoffs": "Requires extra LLM call (latency + cost). Works well when query is short/vague but answer is detailed.",
      "references": ["Gao et al. 2022", "https://arxiv.org/abs/2212.10496"]
    },
    {
      "id": "metadata_filtering",
      "name": "Metadata Filtering",
      "category": "retrieval",
      "description": "Filter documents by metadata (date, author, category) before semantic search.",
      "technical": "Pre-filter with SQL-like conditions (e.g., date > 2023, category = 'science') then run vector search on filtered subset.",
      "magnitudes": "Can reduce search space by 10-100x, improving speed and relevance",
      "tradeoffs": "Requires metadata extraction and storage. Adds complexity to indexing pipeline.",
      "references": ["Hybrid search pattern"]
    },
    {
      "id": "chunking_strategy",
      "name": "Child/Parent Chunking",
      "category": "retrieval",
      "description": "Retrieve small chunks for precision, but return larger parent chunks for context.",
      "technical": "Index small chunks (e.g., sentences) for fine-grained matching. On retrieval, return parent chunk (paragraph/section) for LLM context.",
      "magnitudes": "Child: 100-200 tokens, Parent: 500-1000 tokens",
      "tradeoffs": "Better context for LLM but uses more tokens. Need to track chunk hierarchy.",
      "references": ["RAG best practices"]
    },
    {
      "id": "time_weighted_retrieval",
      "name": "Time-Weighted Retrieval",
      "category": "retrieval",
      "description": "Boost recent documents in search results to prioritize fresh information.",
      "technical": "Multiply similarity score by time decay function: score × exp(-λ × age). λ controls decay rate.",
      "magnitudes": "Typical: boost last week by 2x, last month by 1.5x, etc.",
      "tradeoffs": "Keeps results fresh but may miss older relevant content. Need to tune decay rate.",
      "references": ["Temporal search patterns"]
    },
    {
      "id": "knowledge_graph_rag",
      "name": "Knowledge Graph RAG",
      "category": "retrieval",
      "description": "Use knowledge graph (entities, relations) to augment or replace vector retrieval.",
      "technical": "Extract entities from query → Traverse graph to find related entities/facts → Provide as context to LLM. Can combine with vector search.",
      "magnitudes": "Graph DBs like Neo4j. Typically 1-3 hop traversal.",
      "tradeoffs": "Structured knowledge + reasoning but requires graph construction and maintenance. Good for factual Q&A.",
      "references": ["Neo4j + LLMs", "GraphRAG"]
    },
    {
      "id": "adaptive_rag",
      "name": "Adaptive RAG",
      "category": "retrieval",
      "description": "System decides whether to use vector search, web search, or no retrieval based on query type.",
      "technical": "Router LLM classifies query intent → Route to appropriate retrieval: (1) Vector DB for internal knowledge, (2) Web search for current events, (3) Direct LLM for reasoning.",
      "magnitudes": "Can improve latency (skip retrieval when not needed) and accuracy",
      "tradeoffs": "Adds routing overhead and complexity. Router must be accurate.",
      "references": ["https://github.com/mistralai/cookbook"]
    },
    {
      "id": "kg_agent",
      "name": "Knowledge Graph Agent",
      "category": "retrieval",
      "description": "LLM agent that extracts entities and relations from text to populate knowledge graph.",
      "technical": "Prompt LLM to extract (entity1, relation, entity2) triples from documents → Store in Neo4j/graph DB. Enables structured retrieval and reasoning.",
      "magnitudes": "Can process web pages, PDFs, etc. and build graph incrementally",
      "tradeoffs": "LLM extraction not perfect (hallucinations, missed relations). Requires graph storage and query capabilities.",
      "references": ["https://github.com/camel-ai/camel", "CAMEL Knowledge Graph Agent"]
    },
    {
      "id": "acl",
      "name": "Access Control Lists (ACLs)",
      "category": "retrieval",
      "description": "Security mechanism defining which users can access specific documents and what operations are allowed.",
      "technical": "Each document has ACL: list of (user/group, permissions). Retrieval system filters results based on requesting user's permissions (view, edit, manage).",
      "magnitudes": "Critical for enterprise RAG systems with sensitive data",
      "tradeoffs": "Adds complexity to indexing and retrieval. Must check permissions on every query. Can impact performance.",
      "references": ["Enterprise search best practices"]
    }
  ],
  "links": [
    {
      "source": "multi_head_attention",
      "target": "scaled_dot_product",
      "type": "depends_on",
      "description": "Each attention head internally computes scaled dot-product attention (QK^T/sqrt(d_k) then softmax). Multi-head is just h parallel instances of this core operation."
    },
    {
      "source": "multi_head_attention",
      "target": "attention_complexity",
      "type": "impacts",
      "description": "Running h heads multiplies the attention computation. Total cost is O(h * n^2 * d/h) = O(n^2 * d), so the quadratic bottleneck in sequence length remains."
    },
    {
      "source": "multi_head_attention",
      "target": "layer_norm",
      "type": "related_to",
      "description": "In transformer blocks, layer normalization is applied right before or after multi-head attention (pre-norm vs post-norm). They are architectural neighbors in every block."
    },
    {
      "source": "scaled_dot_product",
      "target": "attention_complexity",
      "type": "impacts",
      "description": "The QK^T matrix multiplication produces an n*n attention matrix, which is the direct source of the O(n^2) memory and compute cost."
    },
    {
      "source": "scaled_dot_product",
      "target": "numerical_instability",
      "type": "impacts",
      "description": "Without the sqrt(d_k) scaling, dot products grow with dimension, pushing softmax into saturation where gradients vanish. The scaling prevents this but doesn't eliminate all FP16 issues."
    },
    {
      "source": "layer_norm",
      "target": "residual_connections",
      "type": "optimizes",
      "description": "Pre-LayerNorm placed before the sublayer ensures the residual path carries an identity gradient, stabilizing training for very deep networks."
    },
    {
      "source": "layer_norm",
      "target": "numerical_instability",
      "type": "optimizes",
      "description": "By normalizing activations to zero mean and unit variance at every layer, LayerNorm prevents activation magnitudes from exploding or vanishing across depth."
    },
    {
      "source": "layer_norm",
      "target": "mixed_precision",
      "type": "related_to",
      "description": "LayerNorm is often kept in FP32 even during mixed-precision training because normalization statistics are sensitive to precision loss."
    },
    {
      "source": "residual_connections",
      "target": "activation_memory",
      "type": "impacts",
      "description": "Skip connections require storing input activations at each block for the addition during forward pass, and for backpropagation, increasing total activation memory."
    },
    {
      "source": "kv_cache",
      "target": "context_window",
      "type": "optimizes",
      "description": "By caching previously computed K and V tensors, KV cache makes autoregressive generation over long contexts feasible — each new token only computes one new K,V pair instead of recomputing all."
    },
    {
      "source": "kv_cache",
      "target": "weight_memory",
      "type": "trades_off",
      "description": "KV cache trades compute savings for memory: storing K,V for all layers and all past tokens grows linearly with sequence length and can rival model weight memory for long contexts."
    },
    {
      "source": "kv_cache",
      "target": "latency",
      "type": "optimizes",
      "description": "Without KV cache, each new token would require recomputing attention over the full sequence. Caching reduces per-token latency from O(n*d) to O(d) for the K,V computation."
    },
    {
      "source": "flash_attention",
      "target": "attention_complexity",
      "type": "optimizes",
      "description": "Flash Attention doesn't reduce asymptotic FLOPs but dramatically reduces memory reads/writes by fusing QK^T, softmax, and V multiplication in a single GPU kernel pass using tiling."
    },
    {
      "source": "flash_attention",
      "target": "weight_memory",
      "type": "optimizes",
      "description": "By never materializing the full n*n attention matrix in HBM, Flash Attention reduces peak memory from O(n^2) to O(n), enabling longer sequences in the same GPU memory."
    },
    {
      "source": "flash_attention",
      "target": "latency",
      "type": "optimizes",
      "description": "The main speedup comes from reducing HBM bandwidth bottleneck: fewer memory round-trips means 2-4x faster wall-clock time for the attention computation."
    },
    {
      "source": "sparse_attention",
      "target": "attention_complexity",
      "type": "optimizes",
      "description": "By restricting which token pairs attend to each other (local windows, global tokens, random connections), sparse attention reduces the n^2 factor to n*sqrt(n) or n*log(n)."
    },
    {
      "source": "sparse_attention",
      "target": "context_window",
      "type": "optimizes",
      "description": "Reduced memory complexity allows processing much longer sequences — models like BigBird and Longformer handle 4k-16k tokens where standard attention would OOM."
    },
    {
      "source": "sliding_window_attention",
      "target": "sparse_attention",
      "type": "related_to",
      "description": "Sliding window is the simplest sparse attention pattern: each token only attends to its k nearest neighbors. Used in Mistral and as a building block in more complex sparse schemes."
    },
    {
      "source": "sliding_window_attention",
      "target": "attention_complexity",
      "type": "optimizes",
      "description": "Reduces attention from O(n^2) to O(n*w) where w is window size. For w=512 and n=32k, that's a 64x reduction in attention computation."
    },
    {
      "source": "context_window",
      "target": "attention_complexity",
      "type": "impacts",
      "description": "Doubling the context window quadruples the attention matrix size. This quadratic relationship is the fundamental reason context length is limited."
    },
    {
      "source": "context_window",
      "target": "oom",
      "type": "impacts",
      "description": "Long contexts cause OOM because both the attention matrix (O(n^2)) and KV cache (O(n*d*layers)) grow with sequence length, easily exceeding GPU memory."
    },
    {
      "source": "batching_strategy",
      "target": "throughput",
      "type": "optimizes",
      "description": "Batching amortizes fixed GPU overhead across multiple requests. Dynamic batching and continuous batching (like vLLM's PagedAttention) maximize GPU utilization."
    },
    {
      "source": "batching_strategy",
      "target": "latency",
      "type": "trades_off",
      "description": "Larger batches improve throughput but increase per-request latency — each request waits for the slowest in the batch, and GPU memory is shared among all batch members."
    },
    {
      "source": "batching_strategy",
      "target": "weight_memory",
      "type": "impacts",
      "description": "Each request in a batch needs its own KV cache and activation memory. Larger batches multiply the memory footprint beyond just model weights."
    },
    {
      "source": "attention_complexity",
      "target": "oom",
      "type": "impacts",
      "description": "The O(n^2) attention matrix is often the first thing to cause OOM when scaling sequence length. For n=8192 in FP16, the attention matrix alone is ~128MB per head per layer."
    },
    {
      "source": "attention_complexity",
      "target": "latency",
      "type": "impacts",
      "description": "Quadratic attention dominates inference time for long sequences. Even with KV cache, the attention score computation still scales linearly with cached context length."
    },
    {
      "source": "activation_memory",
      "target": "oom",
      "type": "impacts",
      "description": "During training, all intermediate activations must be stored for backpropagation. This typically requires 10-20x the model size in memory, making it the primary OOM cause in training."
    },
    {
      "source": "weight_memory",
      "target": "oom",
      "type": "impacts",
      "description": "A 70B parameter model needs 140GB in FP16 just for weights — already exceeding a single A100 (80GB). This is the baseline memory floor before any computation."
    },
    {
      "source": "gradient_memory",
      "target": "oom",
      "type": "impacts",
      "description": "Training requires storing gradients (same size as weights) plus optimizer states (2x for Adam momentum+variance). A 7B model in FP32 needs ~84GB just for weights+gradients+optimizer."
    },
    {
      "source": "mixed_precision",
      "target": "loss_scaling",
      "type": "depends_on",
      "description": "FP16 has limited dynamic range — small gradients underflow to zero. Loss scaling multiplies the loss by a large constant before backprop, keeping gradients representable in FP16."
    },
    {
      "source": "mixed_precision",
      "target": "weight_memory",
      "type": "optimizes",
      "description": "Using FP16 for forward/backward passes halves activation and gradient memory. Master weights stay in FP32 for numerical accuracy, but the bulk of computation uses less memory."
    },
    {
      "source": "mixed_precision",
      "target": "numerical_instability",
      "type": "trades_off",
      "description": "FP16 has only 5 exponent bits vs FP32's 8, making overflow/underflow much more likely. This is the core trade-off: 2x memory savings vs risk of NaN/divergence."
    },
    {
      "source": "loss_scaling",
      "target": "numerical_instability",
      "type": "optimizes",
      "description": "By scaling gradients up before FP16 computation and down after, loss scaling preserves small gradient values that would otherwise underflow to zero and cause training instability."
    },
    {
      "source": "quantization",
      "target": "weight_memory",
      "type": "optimizes",
      "description": "INT8 quantization cuts weight memory by 4x vs FP32 (1 byte vs 4 bytes per param). INT4 (GPTQ, AWQ) achieves 8x reduction, making 70B models fit on consumer GPUs."
    },
    {
      "source": "quantization",
      "target": "latency",
      "type": "optimizes",
      "description": "Smaller weights mean less memory bandwidth needed to feed the GPU. Since LLM inference is typically memory-bandwidth-bound, quantization directly improves tokens/second."
    },
    {
      "source": "rlhf",
      "target": "post_training",
      "type": "related_to",
      "description": "RLHF is the final stage of post-training: after supervised fine-tuning (SFT) aligns format, RLHF optimizes for human preferences on subjective qualities like helpfulness and safety."
    },
    {
      "source": "post_training",
      "target": "gradient_memory",
      "type": "impacts",
      "description": "Fine-tuning still requires gradient computation and optimizer states. Even with LoRA (low-rank adaptation), some gradient memory is needed for the trainable parameters."
    },
    {
      "source": "tensor_parallelism",
      "target": "weight_memory",
      "type": "optimizes",
      "description": "TP splits weight matrices across GPUs — each device stores only 1/N of each layer's parameters. Essential when a single layer's weights exceed one GPU's memory."
    },
    {
      "source": "tensor_parallelism",
      "target": "communication_overhead",
      "type": "impacts",
      "description": "TP requires all-reduce communication after every layer's forward pass to recombine partial results. This makes fast interconnects (NVLink) critical for TP performance."
    },
    {
      "source": "tensor_parallelism",
      "target": "latency",
      "type": "trades_off",
      "description": "While TP reduces per-GPU compute, the mandatory all-reduce synchronization after each layer adds latency. With slow interconnects, TP can actually increase total latency."
    },
    {
      "source": "pipeline_parallelism",
      "target": "weight_memory",
      "type": "optimizes",
      "description": "PP distributes model layers across GPUs sequentially. Each device only stores a subset of layers, reducing per-GPU weight memory proportionally to the number of pipeline stages."
    },
    {
      "source": "pipeline_parallelism",
      "target": "throughput",
      "type": "optimizes",
      "description": "Micro-batching keeps all pipeline stages busy simultaneously, achieving near-linear throughput scaling. Once the pipeline is full, multiple micro-batches are in flight."
    },
    {
      "source": "pipeline_parallelism",
      "target": "latency",
      "type": "trades_off",
      "description": "A single request must traverse all pipeline stages sequentially, so latency equals the sum of all stage times. Pipeline bubbles at start/end also waste compute."
    },
    {
      "source": "data_parallelism",
      "target": "throughput",
      "type": "optimizes",
      "description": "DP replicates the model on N GPUs, each processing different data. Throughput scales linearly with GPU count, limited only by gradient synchronization overhead."
    },
    {
      "source": "data_parallelism",
      "target": "communication_overhead",
      "type": "impacts",
      "description": "DP requires all-reduce of gradients across all replicas after each backward pass. For large models, this gradient communication can take 30-50% of total training time."
    },
    {
      "source": "data_parallelism",
      "target": "gradient_memory",
      "type": "impacts",
      "description": "Standard DP stores full gradients and optimizer states on each GPU. ZeRO (DeepSpeed) shards these across devices, reducing per-GPU gradient memory by the parallelism degree."
    },
    {
      "source": "communication_overhead",
      "target": "flops",
      "type": "trades_off",
      "description": "Time spent transferring data between GPUs is time not spent computing. High communication overhead reduces effective FLOP utilization (MFU), wasting expensive GPU cycles."
    },
    {
      "source": "communication_overhead",
      "target": "throughput",
      "type": "impacts",
      "description": "Network bandwidth and latency between devices caps maximum throughput in distributed training. Faster interconnects (NVLink: 900 GB/s vs PCIe: 64 GB/s) directly increase throughput."
    },
    {
      "source": "latency",
      "target": "throughput",
      "type": "trades_off",
      "description": "The fundamental serving trade-off: batching more requests improves GPU utilization (throughput) but each request waits longer (latency). Continuous batching helps but doesn't eliminate this."
    },
    {
      "source": "memorization",
      "target": "perplexity",
      "type": "related_to",
      "description": "Models with low perplexity on training data may have memorized it rather than learned generalizable patterns. Perplexity on held-out data helps distinguish genuine understanding from memorization."
    },
    {
      "source": "hallucination_detection",
      "target": "bleu_rouge",
      "type": "related_to",
      "description": "BLEU/ROUGE measure n-gram overlap with reference text — low scores can indicate hallucinated content that diverges from factual references, though these metrics miss semantic hallucinations."
    },
    {
      "source": "toxicity_detection",
      "target": "hallucination_detection",
      "type": "related_to",
      "description": "Both are output quality monitoring tasks. Toxic outputs and hallucinations are different failure modes but share similar detection infrastructure (classifiers, human review pipelines)."
    },
    {
      "source": "bm25",
      "target": "tf_idf",
      "type": "depends_on",
      "description": "BM25 is an evolution of TF-IDF that adds document length normalization and term frequency saturation. It builds on the same sparse term-matching paradigm."
    },
    {
      "source": "sparse_retrieval",
      "target": "tf_idf",
      "type": "depends_on",
      "description": "TF-IDF is the foundational sparse retrieval method — documents become high-dimensional sparse vectors where each non-zero entry represents a term's importance."
    },
    {
      "source": "sparse_retrieval",
      "target": "bm25",
      "type": "depends_on",
      "description": "BM25 is the standard production sparse retrieval algorithm. It's what Elasticsearch, Lucene, and Solr use under the hood for keyword-based document ranking."
    },
    {
      "source": "hybrid_retrieval",
      "target": "dense_retrieval",
      "type": "depends_on",
      "description": "Hybrid retrieval runs dense (semantic) search as one of its two retrieval paths, capturing meaning-based matches that keyword search would miss."
    },
    {
      "source": "hybrid_retrieval",
      "target": "sparse_retrieval",
      "type": "depends_on",
      "description": "Hybrid retrieval runs sparse (keyword) search as its second path, catching exact term matches and rare entity names that dense models might not encode well."
    },
    {
      "source": "dense_retrieval",
      "target": "ann_search",
      "type": "depends_on",
      "description": "Dense retrieval produces embedding vectors that must be searched efficiently. ANN algorithms (HNSW, IVF) make this feasible at billion-document scale without brute-force comparison."
    },
    {
      "source": "dense_retrieval",
      "target": "multi_head_attention",
      "type": "related_to",
      "description": "Dense retrieval encoders (BERT, sentence transformers) use multi-head attention to produce embeddings. The quality of retrieval depends directly on the attention mechanism's ability to capture semantics."
    },
    {
      "source": "faiss",
      "target": "ann_search",
      "type": "depends_on",
      "description": "FAISS implements multiple ANN algorithms (flat, IVF, HNSW, PQ) and provides GPU-accelerated search. It's the most widely used library for production-scale vector similarity search."
    },
    {
      "source": "faiss",
      "target": "weight_memory",
      "type": "impacts",
      "description": "FAISS indices storing billions of vectors consume significant memory. Product quantization can compress vectors but the index itself can require tens of GB of RAM or GPU memory."
    },
    {
      "source": "ann_search",
      "target": "latency",
      "type": "optimizes",
      "description": "ANN trades a small accuracy loss for 100-1000x speed improvement over brute-force search. HNSW can search millions of vectors in under 1ms, making real-time retrieval feasible."
    },
    {
      "source": "hyde",
      "target": "dense_retrieval",
      "type": "depends_on",
      "description": "HyDE generates a hypothetical answer, then uses dense retrieval to find real documents similar to that answer. The key insight: answer-to-document similarity is often better than query-to-document."
    },
    {
      "source": "metadata_filtering",
      "target": "hybrid_retrieval",
      "type": "related_to",
      "description": "Metadata filters (date, source, category) are commonly applied before or alongside hybrid search to narrow the search space and improve result relevance."
    },
    {
      "source": "chunking_strategy",
      "target": "context_window",
      "type": "impacts",
      "description": "Chunk sizes must be calibrated to the LLM's context window. Too-large chunks waste context tokens; too-small chunks lose surrounding context needed for comprehension."
    },
    {
      "source": "chunking_strategy",
      "target": "dense_retrieval",
      "type": "impacts",
      "description": "Chunk size directly affects embedding quality. Smaller chunks produce more focused embeddings (better precision) while larger chunks capture more context (better recall)."
    },
    {
      "source": "chunking_strategy",
      "target": "sparse_retrieval",
      "type": "impacts",
      "description": "Chunk size affects term frequency statistics. Very small chunks may not have enough terms for meaningful BM25 scores; very large chunks dilute important terms."
    },
    {
      "source": "time_weighted_retrieval",
      "target": "dense_retrieval",
      "type": "related_to",
      "description": "Time weighting modifies dense retrieval scores with a recency decay function, boosting recent documents. Useful when information freshness matters (news, docs, changelogs)."
    },
    {
      "source": "time_weighted_retrieval",
      "target": "sparse_retrieval",
      "type": "related_to",
      "description": "Time decay can also be applied to sparse retrieval scores. Common pattern: multiply BM25 score by exp(-lambda * document_age) to prioritize recent content."
    },
    {
      "source": "knowledge_graph_rag",
      "target": "kg_agent",
      "type": "depends_on",
      "description": "Knowledge Graph RAG requires an agent to extract entities and relations from documents into a structured graph. Without the extraction agent, there's no graph to query."
    },
    {
      "source": "knowledge_graph_rag",
      "target": "hybrid_retrieval",
      "type": "related_to",
      "description": "KG RAG can complement hybrid retrieval: use vector/keyword search for passage retrieval, then traverse the knowledge graph for structured facts and multi-hop reasoning."
    },
    {
      "source": "knowledge_graph_rag",
      "target": "hallucination_detection",
      "type": "optimizes",
      "description": "Knowledge graphs provide verifiable facts (entity-relation-entity triples) that can be cross-checked against LLM outputs, making hallucination detection more reliable."
    },
    {
      "source": "adaptive_rag",
      "target": "dense_retrieval",
      "type": "depends_on",
      "description": "Adaptive RAG uses dense retrieval as one of its routing targets — selected when the query requires semantic understanding of internal knowledge base content."
    },
    {
      "source": "adaptive_rag",
      "target": "sparse_retrieval",
      "type": "depends_on",
      "description": "Sparse retrieval is another routing target in adaptive RAG, chosen for queries with specific keywords, entity names, or technical terms that need exact matching."
    },
    {
      "source": "adaptive_rag",
      "target": "hybrid_retrieval",
      "type": "depends_on",
      "description": "Adaptive RAG can route to hybrid retrieval for complex queries that benefit from both semantic and keyword matching, getting the best coverage."
    },
    {
      "source": "adaptive_rag",
      "target": "latency",
      "type": "optimizes",
      "description": "By skipping retrieval entirely for queries the LLM can answer directly (reasoning, math), adaptive RAG avoids unnecessary search latency and reduces average response time."
    },
    {
      "source": "acl",
      "target": "dense_retrieval",
      "type": "impacts",
      "description": "ACLs add a permission-checking step to dense retrieval: after finding nearest vectors, results must be filtered by user permissions, potentially reducing the effective result set."
    },
    {
      "source": "acl",
      "target": "sparse_retrieval",
      "type": "impacts",
      "description": "ACLs must be enforced on sparse retrieval results too. In enterprise settings, a user should never see documents they don't have access to, even if they're highly relevant."
    },
    {
      "source": "hybrid_retrieval",
      "target": "hallucination_detection",
      "type": "optimizes",
      "description": "Hybrid retrieval provides more complete source material to the LLM by catching both semantic and keyword matches, giving hallucination detectors better reference material to verify against."
    },
    {
      "source": "dense_retrieval",
      "target": "latency",
      "type": "impacts",
      "description": "Dense retrieval adds encoding latency (embedding the query) plus search latency (ANN lookup). For real-time applications, this 10-50ms overhead can be significant."
    },
    {
      "source": "hybrid_retrieval",
      "target": "latency",
      "type": "impacts",
      "description": "Hybrid retrieval roughly doubles search latency vs single-method retrieval since it runs both dense and sparse searches, plus fusion scoring. Parallel execution helps but doesn't eliminate the cost."
    }
  ]
}
